#!/usr/bin/env python3
"""
Simplified model loading test - checks if basic setup is working
"""


def test_imports():
    """Test if required packages are installed"""
    try:
        import torch

        print(f"âœ… PyTorch version: {torch.__version__}")

        import transformers

        print(f"âœ… Transformers version: {transformers.__version__}")

        from transformers import (
            AutoTokenizer,
            AutoModelForSeq2SeqLM,
            AutoModelForCausalLM,
        )

        print("âœ… All required imports successful")

        # Check CUDA availability
        if torch.cuda.is_available():
            print(f"ğŸš€ CUDA available: {torch.cuda.get_device_name(0)}")
        else:
            print("ğŸ’» Using CPU (CUDA not available)")

        return True
    except ImportError as e:
        print(f"âŒ Import error: {e}")
        return False


def test_flan_t5_loading():
    """Test if Flan-T5 can be loaded"""
    try:
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

        print("ğŸ“¥ Attempting to load Flan-T5...")
        tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
        model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

        # Test a simple query
        prompt = (
            "Extract keywords from this text: Python developer with React experience"
        )
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
        outputs = model.generate(**inputs, max_new_tokens=20)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        print(f"âœ… Flan-T5 loaded and tested successfully")
        print(f"ğŸ“ Test response: {response}")
        return True

    except Exception as e:
        print(f"âŒ Flan-T5 loading failed: {e}")
        return False


def test_llama2_availability():
    """Test if LLaMA 2 model can be accessed"""
    try:
        from transformers import AutoTokenizer

        print("ğŸ“¥ Checking LLaMA 2 availability...")
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
        print("âœ… LLaMA 2 tokenizer loaded successfully")

        # Note: We're not loading the full model here as it's large
        print("â„¹ï¸  Full model loading will be tested when backend starts")
        return True

    except Exception as e:
        print(f"âš ï¸ LLaMA 2 not available: {e}")
        print("ğŸ“ This is expected if you don't have access to the model")
        return False


def main():
    print("ğŸ”§ Running Environment Setup Test")
    print("=" * 50)

    imports_ok = test_imports()
    if not imports_ok:
        print("\nâŒ Basic setup failed. Please install requirements:")
        print("pip install torch transformers accelerate sentencepiece")
        return

    print("\nğŸ§ª Testing Model Loading...")
    flan_ok = test_flan_t5_loading()
    llama_ok = test_llama2_availability()

    print("\nğŸ“Š Results:")
    print(f"Environment Setup: {'âœ…' if imports_ok else 'âŒ'}")
    print(f"Flan-T5 (CV Analysis): {'âœ…' if flan_ok else 'âŒ'}")
    print(f"LLaMA 2 (Chat): {'âœ…' if llama_ok else 'âš ï¸'}")

    if flan_ok:
        print("\nğŸ‰ Setup successful! Flan-T5 is ready for CV analysis.")
        if llama_ok:
            print("ğŸš€ LLaMA 2 is also available for enhanced chat!")
        else:
            print("ğŸ’¡ LLaMA 2 will fallback to Flan-T5 for chat interactions.")
        print("\nâ–¶ï¸  You can now start the backend with: python app.py")
    else:
        print("\nâš ï¸ Setup incomplete. Please check your environment.")


if __name__ == "__main__":
    main()
